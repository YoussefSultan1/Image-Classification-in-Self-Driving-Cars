{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YoussefSultan1/Image-Classification-in-Self-Driving-Cars/blob/main/Image_Classification_in_Self_Driving_Cars.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RQlayGTuwBGx",
      "metadata": {
        "id": "RQlayGTuwBGx"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "q7GVIK_ZYP28",
      "metadata": {
        "id": "q7GVIK_ZYP28"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oxKxKx1C6nds",
      "metadata": {
        "id": "oxKxKx1C6nds"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3VLX_crP4sbQ",
      "metadata": {
        "id": "3VLX_crP4sbQ"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!ls ~/.kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VU4xXl8r6yTJ",
      "metadata": {
        "id": "VU4xXl8r6yTJ"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download -d valentynsichkar/traffic-signs-preprocessed -p '/content/drive/MyDrive/project/dataset.zip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Q-So2dYYYtRa",
      "metadata": {
        "id": "Q-So2dYYYtRa"
      },
      "outputs": [],
      "source": [
        "!unzip -q '/content/drive/MyDrive/project/dataset.zip/traffic-signs-preprocessed.zip' -d '/content/drive/MyDrive/project/dataset'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f4ec326",
      "metadata": {
        "id": "0f4ec326"
      },
      "source": [
        "### Printing the list of files in a directory\n",
        "This code imports necessary libraries and defines the input directory. It uses the `os.walk()` function to iterate over all files in a specified directory and its subdirectories, printing their full path. It then prints a list of all files in the specified directory. This is a useful technique for quickly inspecting the contents of a directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58e27cbd",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2023-04-22T18:55:12.911838Z",
          "iopub.status.busy": "2023-04-22T18:55:12.910864Z",
          "iopub.status.idle": "2023-04-22T18:55:12.924383Z",
          "shell.execute_reply": "2023-04-22T18:55:12.922813Z",
          "shell.execute_reply.started": "2023-04-22T18:55:12.911785Z"
        },
        "id": "58e27cbd",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import warnings\n",
        "\n",
        "# Ignore all warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\"Clipping input data to the valid range for imshow\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import (AvgPool2D, BatchNormalization, Conv2D, Dense,\n",
        "                          Dropout, Flatten, MaxPool2D, Reshape)\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "# Define input directory\n",
        "INPUT_DIR = '/content/drive/MyDrive/project/dataset'\n",
        "\n",
        "# Print all files under the input directory\n",
        "for root, dirs, files in os.walk(INPUT_DIR):\n",
        "    for filename in files:\n",
        "        print(os.path.join(root, filename))\n",
        "\n",
        "# Print list of files under the input directory\n",
        "print(os.listdir(INPUT_DIR))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f488d74",
      "metadata": {
        "id": "4f488d74"
      },
      "source": [
        "### Loading and preprocessing the dataset\n",
        "This script loads traffic sign image data from a pickle file, and prepares it for use in the Keras deep learning framework. The script uses the `to_categorical()` function to convert class vectors to binary class matrices, and transposes the image data to make channels come at the end. Finally, it prints the shapes of the loaded data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1293c529",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-22T18:57:21.666175Z",
          "iopub.status.busy": "2023-04-22T18:57:21.664907Z",
          "iopub.status.idle": "2023-04-22T18:57:40.845429Z",
          "shell.execute_reply": "2023-04-22T18:57:40.844578Z",
          "shell.execute_reply.started": "2023-04-22T18:57:21.666126Z"
        },
        "id": "1293c529"
      },
      "outputs": [],
      "source": [
        "# Open pickle file for reading in binary mode\n",
        "with open('/content/drive/MyDrive/project/dataset/data2.pickle', 'rb') as file:\n",
        "    # Load data from pickle file\n",
        "    data = pickle.load(file, encoding='latin1')\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "num_classes = 43\n",
        "data['y_train'] = to_categorical(data['y_train'], num_classes=num_classes)\n",
        "data['y_validation'] = to_categorical(data['y_validation'], num_classes=num_classes)\n",
        "\n",
        "# Transpose arrays to make channels come at the end\n",
        "data['x_train'] = data['x_train'].transpose((0, 2, 3, 1))\n",
        "data['x_validation'] = data['x_validation'].transpose((0, 2, 3, 1))\n",
        "data['x_test'] = data['x_test'].transpose((0, 2, 3, 1))\n",
        "\n",
        "# Reduce the size of the training dataset to 60%, to avoid RAM overloading\n",
        "train_size = int(0.6 * len(data['x_train']))\n",
        "data['x_train'] = data['x_train'][:train_size]\n",
        "data['y_train'] = data['y_train'][:train_size]\n",
        "\n",
        "# Print shapes of loaded data from file\n",
        "for key, value in data.items():\n",
        "    if key == 'labels':\n",
        "        print(f\"{key}: {len(value)}\")\n",
        "    else:\n",
        "        print(f\"{key}: {value.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98f8beb1",
      "metadata": {
        "id": "98f8beb1"
      },
      "source": [
        "### Visualizing examples of training data in grid format using matplotlib\n",
        "This code creates a grid of example images from the training data using the `convert_to_grid()` function. The `matplotlib` library is used to display the grid and save it as a PNG file. The function takes a 4D tensor as input and scales the pixel values to the range [0, 255]. The resulting grid is then plotted using imshow and the `gray` color map. The resulting plot is displayed and saved as 'training_examples.png'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "961d87e2",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-22T18:59:38.388410Z",
          "iopub.status.busy": "2023-04-22T18:59:38.387623Z",
          "iopub.status.idle": "2023-04-22T18:59:38.908368Z",
          "shell.execute_reply": "2023-04-22T18:59:38.907308Z",
          "shell.execute_reply.started": "2023-04-22T18:59:38.388369Z"
        },
        "id": "961d87e2"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Define function to convert a 4D tensor to a grid\n",
        "def convert_to_grid(x_input):\n",
        "    N, H, W, C = x_input.shape\n",
        "    grid_size = int(np.ceil(np.sqrt(N)))\n",
        "    grid_height = H * grid_size + 1 * (grid_size - 1)\n",
        "    grid_width = W * grid_size + 1 * (grid_size - 1)\n",
        "    grid = np.zeros((grid_height, grid_width, C)) + 255\n",
        "    next_idx = 0\n",
        "    y0, y1 = 0, H\n",
        "    for y in range(grid_size):\n",
        "        x0, x1 = 0, W\n",
        "        for x in range(grid_size):\n",
        "            if next_idx < N:\n",
        "                img = x_input[next_idx]\n",
        "                low, high = np.min(img), np.max(img)\n",
        "                grid[y0:y1, x0:x1] = 255.0 * (img - low) / (high - low)\n",
        "                next_idx += 1\n",
        "            x0 += W + 1\n",
        "            x1 += W + 1\n",
        "        y0 += H + 1\n",
        "        y1 += H + 1\n",
        "    return grid\n",
        "\n",
        "# Define examples and plot them\n",
        "examples = data['x_train'][:49, :, :, :]\n",
        "fig = plt.figure()\n",
        "grid = convert_to_grid(examples)\n",
        "plt.imshow(grid.astype('uint8'), cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.gcf().set_size_inches(15, 15)\n",
        "plt.title('Some examples of training data', fontsize=28)\n",
        "plt.show()\n",
        "\n",
        "# Save the plot\n",
        "fig.savefig('/content/drive/MyDrive/project/training_examples.png')\n",
        "plt.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "228ae9e7",
      "metadata": {
        "id": "228ae9e7"
      },
      "source": [
        "### Simple CNN model using Keras\n",
        "The code defines a Sequential model, which includes a convolutional layer, a max pooling layer, a flatten layer, and two dense layers. The model is then compiled using Adam optimizer, categorical crossentropy loss, and accuracy metric. The model is trained for 15 epochs with a learning rate annealer callback."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fccd1c6d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-22T19:02:54.762245Z",
          "iopub.status.busy": "2023-04-22T19:02:54.761689Z",
          "iopub.status.idle": "2023-04-22T19:04:03.745511Z",
          "shell.execute_reply": "2023-04-22T19:04:03.744307Z",
          "shell.execute_reply.started": "2023-04-22T19:02:54.762190Z"
        },
        "id": "fccd1c6d"
      },
      "outputs": [],
      "source": [
        "# Define a Sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Add a convolutional layer with 32 filters, 3x3 kernel size, and ReLU activation\n",
        "model.add(Conv2D(32, kernel_size=3, padding='same', activation='relu', input_shape=(32, 32, 3)))\n",
        "\n",
        "# Add a max pooling layer with 2x2 pool size\n",
        "model.add(MaxPool2D(pool_size=2))\n",
        "\n",
        "# Flatten the output from the convolutional layer\n",
        "model.add(Flatten())\n",
        "\n",
        "# Add a dense layer with 500 units and ReLU activation\n",
        "model.add(Dense(500, activation='relu'))\n",
        "\n",
        "# Add the output layer with 43 units and softmax activation\n",
        "model.add(Dense(43, activation='softmax'))\n",
        "\n",
        "# Compile the model with Adam optimizer, categorical crossentropy loss, and accuracy metric\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Defining the learning rate annealer callback function\n",
        "annealer = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** (x + epochs))\n",
        "\n",
        "# Training the model\n",
        "epochs = 15\n",
        "h = model.fit(data['x_train'][:10], data['y_train'][:10],\n",
        "              batch_size=5, epochs = epochs,\n",
        "              validation_data = (data['x_validation'], data['y_validation']),\n",
        "              callbacks=[annealer], verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f66654c",
      "metadata": {
        "id": "2f66654c"
      },
      "source": [
        "### Print acc and val_acc\n",
        "Printing the training and validation accuracy after training the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb290952",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-22T19:04:03.747881Z",
          "iopub.status.busy": "2023-04-22T19:04:03.747539Z",
          "iopub.status.idle": "2023-04-22T19:04:03.754203Z",
          "shell.execute_reply": "2023-04-22T19:04:03.753113Z",
          "shell.execute_reply.started": "2023-04-22T19:04:03.747849Z"
        },
        "id": "eb290952"
      },
      "outputs": [],
      "source": [
        "print('Epochs={0:d}, training accuracy: {1:.4f}, validation accuracy: {2:.4f}'.\\\n",
        "      format(epochs, max(h.history['accuracy']), max(h.history['val_accuracy'])))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97c40f93",
      "metadata": {
        "id": "97c40f93"
      },
      "source": [
        "### Visualizing Overfitting using Accuracy vs. Epoch plot\n",
        "This code generates a plot of the training and validation accuracy of the model as a function of epoch. The plot helps visualize overfitting on small amount of data. Additionally, the code prints a summary of the training results, including the number of epochs and the maximum training and validation accuracy achieved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tfWCTaRueFOq",
      "metadata": {
        "id": "tfWCTaRueFOq"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set default plot configurations\n",
        "plt.rcParams['figure.figsize'] = (15.0, 5.0)\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "#plt.rcParams['font.family'] = 'Times New Roman'\n",
        "# plt.rcParams['font.family'] = 'Arial'\n",
        "\n",
        "# Plotting accuracy vs. epochs for train and validation sets\n",
        "fig = plt.figure()\n",
        "plt.plot(h.history['accuracy'], '-o', linewidth=3.5)\n",
        "plt.plot(h.history['val_accuracy'], '-o', linewidth=3.5)\n",
        "plt.title('Overfitting small data', fontsize=28)\n",
        "plt.legend(['train', 'validation'], loc='upper left', fontsize='xx-large')\n",
        "plt.xlabel('Epoch', fontsize=20)\n",
        "plt.ylabel('Accuracy', fontsize=20)\n",
        "plt.tick_params(labelsize=20)\n",
        "\n",
        "# Showing the plot\n",
        "plt.show()\n",
        "\n",
        "# Saving the plot\n",
        "fig.savefig('overfitting_small_data.png')\n",
        "plt.close()\n",
        "\n",
        "# Print summary of training results\n",
        "epochs = len(h.history['accuracy'])\n",
        "train_accuracy = max(h.history['accuracy'])\n",
        "val_accuracy = max(h.history['val_accuracy'])\n",
        "print('Training Summary:-\\nEpochs: {0:d}, Training Accuracy: {1:.4f}, Validation Accuracy: {2:.4f}'.format(epochs, train_accuracy, val_accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cc4bf35",
      "metadata": {
        "id": "8cc4bf35"
      },
      "source": [
        "### Building, Training, and Saving a Set of CNN Models with Different Filter Sizes using Keras\n",
        "In this code, we build and train a set of convolutional neural network (CNN) models with Keras, using various filter sizes ranging from 3x3 to 31x31. We train each model for 5 epochs and implement a learning rate scheduler to optimize the learning rate during training. Afterwards, we save the architecture and parameters of each trained model to a file for future use (to ensure that they can be used later without the need for retraining). Finally, we print the training and validation accuracies for each model to compare their performance on a classification task. This code enables us to efficiently explore the effect of different filter sizes on the accuracy of CNN models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f99961d",
      "metadata": {
        "id": "4f99961d"
      },
      "outputs": [],
      "source": [
        "# Define a list of different filter sizes to try\n",
        "filters = [3, 5, 9, 13, 15, 19, 23, 25, 31]\n",
        "\n",
        "# Loop over the filters and create, train, and save models one by one\n",
        "for i in range(len(filters)):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, kernel_size=filters[i], padding='same', activation='relu', input_shape=(32, 32, 3)))\n",
        "    model.add(MaxPool2D(pool_size=2))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(500, activation='relu'))\n",
        "    model.add(Dense(43, activation='softmax'))\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    annealer = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** (x + epochs))\n",
        "    epochs = 5\n",
        "\n",
        "    history = model.fit(data['x_train'], data['y_train'],\n",
        "                        batch_size=8, epochs=epochs,\n",
        "                        validation_data=(data['x_validation'], data['y_validation']),\n",
        "                        callbacks=[annealer], verbose=0)\n",
        "\n",
        "    print('Model with filters: {0:d}x{0:d}, epochs: {1:d}, training accuracy: {2:.4f}, validation accuracy: {3:.4f}'.format(filters[i], epochs, max(history.history['accuracy']), max(history.history['val_accuracy'])))\n",
        "\n",
        "    # Save the model architecture and weights to a file\n",
        "    model_name = \"/content/drive/MyDrive/project/model-{}x{}.h5\".format(filters[i], filters[i])\n",
        "    model.save(model_name)\n",
        "\n",
        "    # Save the history object to a file\n",
        "    with open('/content/drive/MyDrive/project/history-{}x{}.pkl'.format(filters[i], filters[i]), 'wb') as f:\n",
        "        pickle.dump(history.history, f)\n",
        "\n",
        "    # Clear TensorFlow session and variables\n",
        "    tf.keras.backend.clear_session()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "E6EA0JszY36K",
      "metadata": {
        "id": "E6EA0JszY36K"
      },
      "outputs": [],
      "source": [
        "    # Clear TensorFlow session and variables\n",
        "    tf.keras.backend.clear_session()\n",
        "    model = None\n",
        "    history = None\n",
        "\n",
        "    # Clear Python variables\n",
        "    del model\n",
        "    del history\n",
        "\n",
        "    # Collect garbage to release memory\n",
        "    import gc\n",
        "    gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5d5b109",
      "metadata": {
        "id": "e5d5b109"
      },
      "source": [
        "### Loading saved models\n",
        "To load several trained models in Keras, you can use the `load_model()` function from the `keras.models` module. This function loads a saved model and returns a `keras.models.Model` object.\n",
        "After all the models have been trained and saved, we first clear the models list using `models = []`. We then loop over the filters and load each saved model using the `load_model()` function, and assign the loaded model to the same index in the models list using `models.append(model)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bf91401",
      "metadata": {
        "id": "6bf91401"
      },
      "outputs": [],
      "source": [
        "# Redefine the list of different filter sizes\n",
        "filters = [3, 5, 9, 13, 15, 19, 23, 25, 31]\n",
        "\n",
        "# Clear the current models list\n",
        "models = []\n",
        "history = []\n",
        "\n",
        "# Load the saved models and histories and assign them to the models list and history list\n",
        "for i in range(len(filters)):\n",
        "    model_name = \"/content/drive/MyDrive/project/model-{}x{}.h5\".format(filters[i], filters[i])\n",
        "    model = load_model(model_name)\n",
        "    models.append(model)\n",
        "\n",
        "    # Load the history object from file\n",
        "    with open('/content/drive/MyDrive/project/history-{}x{}.pkl'.format(filters[i], filters[i]), 'rb') as f:\n",
        "        history.append(pickle.load(f))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b79f1909",
      "metadata": {
        "id": "b79f1909"
      },
      "source": [
        "### Plotting and printing accuracy results\n",
        "This code imports the necessary libraries and sets the default size of plots and font family. It then plots the history of training and validation accuracy for different sizes of filters using a loop. The legend indicates the size of each filter, and the x-axis shows the epoch. The y-axis shows the accuracy values, and the title of the plot is \"Accuracy for different sizes of filters\".\n",
        "\n",
        "After plotting the accuracy results, the code prints the training and validation accuracy values for each filter size using a loop. The results indicate the maximum accuracy achieved during training and validation for each filter size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b577d4e4",
      "metadata": {
        "id": "b577d4e4"
      },
      "outputs": [],
      "source": [
        "# Plotting history of training and validation accuracy\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (15.0, 15.0) # Setting default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.subplot(2, 1, 1)\n",
        "for i in range(len(history)):\n",
        "    plt.plot(history[i]['accuracy'], '-o', linewidth=3.5)\n",
        "plt.legend(['filter 3', 'filter 5', 'filter 9', 'filter 13', 'filter 15', 'filter 19', 'filter 23', 'filter 25', 'filter 31'], loc='lower right', fontsize='xx-large', borderpad=2)\n",
        "plt.xlabel('Epoch', fontsize=20)\n",
        "plt.ylabel('Training Accuracy', fontsize=20)\n",
        "plt.yscale('linear')  # {\"linear\", \"log\", \"symlog\", \"logit\", ...}\n",
        "plt.ylim(0.85, 1.0)\n",
        "plt.xlim(0.5, 5.3)\n",
        "plt.title('Accuracy for different sizes of filters', fontsize=22)\n",
        "plt.tick_params(labelsize=18)\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "for i in range(len(history)):\n",
        "    plt.plot(history[i]['val_accuracy'], '-o', linewidth=3.5)\n",
        "plt.legend(['filter 3', 'filter 5', 'filter 9', 'filter 13', 'filter 15', 'filter 19', 'filter 23', 'filter 25', 'filter 31'], loc='lower right', fontsize='xx-large', borderpad=2)\n",
        "plt.xlabel('Epoch', fontsize=22)\n",
        "plt.ylabel('Validation Accuracy', fontsize=22)\n",
        "plt.yscale('linear')  # {\"linear\", \"log\", \"symlog\", \"logit\", ...}\n",
        "plt.ylim(0.75, 0.9)\n",
        "plt.xlim(0.5, 5.3)\n",
        "plt.tick_params(labelsize=20)\n",
        "\n",
        "# Showing the plot\n",
        "plt.show()\n",
        "\n",
        "# Saving the plot\n",
        "fig.savefig('/content/drive/MyDrive/project/models_accuracy.png')\n",
        "plt.close()\n",
        "\n",
        "\n",
        "# Showing values of accuracy for different filters\n",
        "for i in range(len(history)):\n",
        "    print('data2 filter {0:d} training accuracy = {1:.5f}'.\\\n",
        "          format(filters[i], np.max(history[i]['accuracy'])))\n",
        "\n",
        "print()\n",
        "\n",
        "for i in range(len(history)):\n",
        "    print('data2 filter {0:d} validation accuracy = {1:.5f}'.\\\n",
        "          format(filters[i], np.max(history[i]['val_accuracy'])))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f91a32e5",
      "metadata": {
        "id": "f91a32e5"
      },
      "source": [
        "### This code calculates the accuracy of different models on a testing dataset\n",
        "The code loops over a list of models (`models`), and for each model, it makes predictions on a testing dataset (`data['x_test']`), gets the predicted classes, calculates the accuracy by comparing predicted classes to true classes (`data['y_test']`), and prints the accuracy along with the number of parameters in the model. The `enumerate` function is used to get both the index (`i`) and model (`model`) in each iteration of the loop. The `np.argmax()` function is used to get the class with the highest probability for each prediction. Finally, the `f-string` syntax is used to print a formatted string with the accuracy and number of parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14ecf3bb",
      "metadata": {
        "id": "14ecf3bb"
      },
      "outputs": [],
      "source": [
        "for i, model in enumerate(models):\n",
        "    # Make predictions on the testing dataset\n",
        "    predictions = model.predict(data['x_test'])\n",
        "    # Get the class with the highest probability for each prediction\n",
        "    predicted_classes = np.argmax(predictions, axis=1)\n",
        "    # Calculate the accuracy by comparing predicted classes to true classes\n",
        "    accuracy = np.mean(predicted_classes == data['y_test'])\n",
        "    # Print the accuracy for each model\n",
        "    print(f\"Testing Filter {filters[i]} on data2 with {model.count_params():,} parameters, testing accuracy = {accuracy:.5f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e18ded5",
      "metadata": {
        "id": "2e18ded5"
      },
      "source": [
        "This code loads a pre-trained model and some test data and calculates the accuracy of the model on the test data. The test data is loaded from four different pickle files, and for each file, the accuracy is calculated and printed out along with the filter size used by the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ecdd00d",
      "metadata": {
        "id": "5ecdd00d"
      },
      "outputs": [],
      "source": [
        "#load data\n",
        "for i in range(4):\n",
        "    with open(f'/content/drive/MyDrive/project/dataset/data{i}.pickle', 'rb') as f:\n",
        "        data = pickle.load(f, encoding='latin1')\n",
        "\n",
        "    # Preparing y_train and y_validation for using in Keras\n",
        "    data['y_train'] = to_categorical(data['y_train'], num_classes=43)\n",
        "    data['y_validation'] = to_categorical(data['y_validation'], num_classes=43)\n",
        "\n",
        "    # Making channels come at the end\n",
        "    data['x_train'] = data['x_train'].transpose(0, 2, 3, 1)\n",
        "    data['x_validation'] = data['x_validation'].transpose(0, 2, 3, 1)\n",
        "    data['x_test'] = data['x_test'].transpose(0, 2, 3, 1)\n",
        "\n",
        "    temp = models[0].predict(data['x_test'])\n",
        "    temp = np.argmax(temp, axis=1)\n",
        "    testing_accuracy = np.mean(temp == data['y_test'])\n",
        "\n",
        "    print('data{0:d} filter 3 testing accuracy = {1:.5f}'.format(i, testing_accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oIA1C2fTwYC8",
      "metadata": {
        "id": "oIA1C2fTwYC8"
      },
      "source": [
        "## Predicting with one image from test dataset\n",
        "This code predicts the class of a single image from the test dataset using a trained model. The code loads the necessary data and model, selects a single image from the test dataset, displays the image, performs a forward pass to obtain the scores for each of the 43 possible classes, determines the class with the highest score, and displays the label for the predicted class. Additionally, the code defines a function for obtaining the text labels for each class, which is used to display the label for the predicted class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb4a2ff5",
      "metadata": {
        "id": "cb4a2ff5"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "# Index for the test image. Note: test dataset has ~12k images\n",
        "index = 528\n",
        "\n",
        "# Preparing image for predicting from test dataset\n",
        "x_input = data['x_test'][index:index+1]\n",
        "print(x_input.shape)\n",
        "y_input = data['y_test'][index:index+1]\n",
        "print(y_input)\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (2.5, 2.5) # Setting default size of plots\n",
        "plt.imshow(x_input[0, :, :, :])\n",
        "plt.axis('off')\n",
        "\n",
        "# Showing the plot\n",
        "plt.show()\n",
        "\n",
        "# Getting scores from forward pass of input image\n",
        "scores = model.predict(x_input)\n",
        "print(scores[0].shape) # (43,)\n",
        "\n",
        "# Scores is given for image with 43 numbers of predictions for each class\n",
        "# Getting only one class with maximum value\n",
        "prediction = np.argmax(scores)\n",
        "print('ClassId:', prediction)\n",
        "\n",
        "# Defining function for getting texts for every class - labels\n",
        "def label_text(file):\n",
        "    # Defining list for saving label in order from 0 to 42\n",
        "    label_list = []\n",
        "\n",
        "    # Reading 'csv' file and getting image's labels\n",
        "    r = pd.read_csv(file)\n",
        "    # Going through all names\n",
        "    for name in r['SignName']:\n",
        "        # Adding from every row second column with name of the label\n",
        "        label_list.append(name)\n",
        "\n",
        "    # Returning resulted list with labels\n",
        "    return label_list\n",
        "\n",
        "\n",
        "# Getting labels\n",
        "labels = label_text('/content/drive/MyDrive/project/dataset/label_names.csv')\n",
        "\n",
        "# Printing label for classified Traffic Sign\n",
        "print('Label:', labels[prediction])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qFNN5ut_23J0",
      "metadata": {
        "id": "qFNN5ut_23J0"
      },
      "outputs": [],
      "source": [
        "# Predicting around ~240 images from test set\n",
        "for index in range(0,12000, 50):\n",
        "    print()\n",
        "    # Preparing image for predicting from test dataset\n",
        "    x_input = data['x_test'][index:index+1]\n",
        "    print(x_input.shape)\n",
        "    y_input = data['y_test'][index:index+1]\n",
        "    print(y_input)\n",
        "\n",
        "    plt.rcParams['figure.figsize'] = (2.5, 2.5) # Setting default size of plots\n",
        "    plt.imshow(x_input[0, :, :, :])\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Showing the plot\n",
        "    plt.show()\n",
        "\n",
        "    # Getting scores from forward pass of input image\n",
        "    scores = model.predict(x_input)\n",
        "    print(scores[0].shape) # (43,)\n",
        "\n",
        "    # Scores is given for image with 43 numbers of predictions for each class\n",
        "    # Getting only one class with maximum value\n",
        "    prediction = np.argmax(scores)\n",
        "    print('ClassId:', prediction)\n",
        "\n",
        "    # Defining function for getting texts for every class - labels\n",
        "    def label_text(file):\n",
        "        # Defining list for saving label in order from 0 to 42\n",
        "        label_list = []\n",
        "\n",
        "        # Reading 'csv' file and getting image's labels\n",
        "        r = pd.read_csv(file)\n",
        "        # Going through all names\n",
        "        for name in r['SignName']:\n",
        "            # Adding from every row second column with name of the label\n",
        "            label_list.append(name)\n",
        "\n",
        "        # Returning resulted list with labels\n",
        "        return label_list\n",
        "\n",
        "\n",
        "    # Getting labels\n",
        "    labels = label_text('/content/drive/MyDrive/project/dataset/label_names.csv')\n",
        "\n",
        "    # Printing label for classified Traffic Sign\n",
        "    print('Label:', labels[prediction])\n",
        "    print()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "x9mAoOSe3e2I",
      "metadata": {
        "id": "x9mAoOSe3e2I"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}